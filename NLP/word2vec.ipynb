{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length : 28210\n",
      "Vocab length : 3508\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from math import floor, ceil\n",
    "import numpy as np\n",
    "data_dir = Path('./data')\n",
    "files = ['wiki_' + color for color in ['black', 'blue', 'brown', 'color', 'cyan', 'green',\n",
    "                                       'grey', 'indigo', 'magenta', 'orange', 'pink', 'purple',\n",
    "                                       'red', 'violet', 'white', 'yellow']]\n",
    "strings = []\n",
    "for file in files:\n",
    "    strings.append(open(data_dir.joinpath(file).with_suffix('.txt')).read())\n",
    "\n",
    "corpus = (' '.join(strings)).split()\n",
    "vocab = set(corpus)\n",
    "vocab_size = len(vocab)\n",
    "print(\"Corpus length : {}\".format(len(corpus)))\n",
    "print(\"Vocab length : {}\".format(len(vocab)))\n",
    "\n",
    "word_index = {}\n",
    "index_word = {}\n",
    "# create word indices\n",
    "for i, word in enumerate(vocab):\n",
    "    word_index[word] = i\n",
    "    index_word[i] = word\n",
    "\n",
    "# create input and output samples\n",
    "# window iterator\n",
    "\n",
    "\n",
    "def windows(corpus, window_len=3):\n",
    "    corpus_len = len(corpus)\n",
    "    if corpus_len < window_len:\n",
    "        raise ValueError(\"Corpus length cannot be smaller than window length\")\n",
    "    half = int(floor(window_len/2))\n",
    "    if corpus_len % 2:  # even\n",
    "        pre_pad = half - 1\n",
    "        post_pad = half\n",
    "    else:  # odd\n",
    "        pre_pad = half\n",
    "        post_pad = half\n",
    "    for i in range(pre_pad, corpus_len - post_pad):\n",
    "        yield corpus[i-pre_pad: i + post_pad + 1]\n",
    "\n",
    "# Creates one hot vectors for words\n",
    "\n",
    "\n",
    "def get_one_hot(index, vocab_size):\n",
    "    zeros = np.zeros(vocab_size)\n",
    "    zeros[index] = 1.\n",
    "    return zeros\n",
    "\n",
    "\n",
    "# hardcoding for window_size = 3\n",
    "vI = []\n",
    "vO1 = []\n",
    "vO2 = []\n",
    "\n",
    "for window in windows(corpus):\n",
    "    # hard coding for window_size=3\n",
    "    vI.append(get_one_hot(word_index[window[1]], vocab_size))\n",
    "    vO1.append(get_one_hot(word_index[window[0]], vocab_size))\n",
    "    vO2.append(get_one_hot(word_index[window[2]], vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vnpI = np.vstack(vI)\n",
    "vnpO1 = np.vstack(vO1)\n",
    "vnpO2 = np.vstack(vO2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Input: (28208, 3508)\n",
      "Shape of Output-1: (28208, 3508)\n",
      "Shape of Output-2: (28208, 3508)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Input: {}\".format(vnpI.shape))\n",
    "print(\"Shape of Output-1: {}\".format(vnpO1.shape))\n",
    "print(\"Shape of Output-2: {}\".format(vnpO2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "\n",
    "We need the following components to implement the core word2vec model in tensorflow and as we will see, these components can seen as layers:\n",
    "\n",
    "1. **The input word embeddings (weights)**: These weights, represented as a matrix $\\boldsymbol{W_i}$, transform the onehot encoded word representation $\\boldsymbol{i}$ into the distributed wordvector representation $\\boldsymbol{v}$.\n",
    " \n",
    "2. **The output word embeddings (set of weights)**: These set of matrices $\\boldsymbol{W_{o_1}}, \\boldsymbol{W_{o_2}}, ...$, hold the embeddings of words when they are in the context i.e., output. The number depends on the size of the context that we choose for the model.\n",
    "\n",
    "3. **Softmax**: To transform the dot product scores into probabilities.\n",
    "\n",
    "4. **Negative log-likelihood/ Cross-entropy loss**: The loss functions for the optimizer.\n",
    "\n",
    "![Model](word2vec_model_structure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-deb4b1b272e5>:18: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "embd_dim = 10\n",
    "# create placeholder to feed the input\n",
    "I = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "O1 = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "O2 = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "# Input and output embeddings\n",
    "Wi = tf.get_variable(\"Wi\", shape=(vocab_size, embd_dim))\n",
    "Wo1 = tf.get_variable(\"Wo1\", shape=(embd_dim, vocab_size))\n",
    "Wo2 = tf.get_variable(\"Wo2\", shape=(embd_dim, vocab_size))\n",
    "\n",
    "# create the model\n",
    "Ei = tf.matmul(I, Wi)\n",
    "So1 = tf.matmul(Ei, Wo1)\n",
    "So2 = tf.matmul(Ei,Wo2)\n",
    "#Po1 = tf.nn.softmax(So1)\n",
    "#Po2 = tf.nn.softmax(So2)\n",
    "loss1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=So1, labels=O1), name=\"loss1\")\n",
    "loss2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=So2, labels=O2), name=\"loss2\")\n",
    "loss = tf.add(loss1, loss2, name=\"total_loss\")\n",
    "training_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "num_samples = vnpI.shape[0]\n",
    "num_steps = epochs*ceil(num_samples/batch_size)\n",
    "\n",
    "\n",
    "def shuffle_in_unison(a, b, c):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p], c[p]\n",
    "\n",
    "\n",
    "def train_batch(I, O1, O2, batch_size=64, steps=1000):\n",
    "    for i in range(steps):\n",
    "        I_batch = np.take(\n",
    "            I, range(batch_size*i, batch_size*(i+1)), axis=0, mode='wrap')\n",
    "        O1_batch = np.take(\n",
    "            O1, range(batch_size*i, batch_size*(i+1)), axis=0, mode='wrap')\n",
    "        O2_batch = np.take(\n",
    "            O2, range(batch_size*i, batch_size*(i+1)), axis=0, mode='wrap')\n",
    "        yield shuffle_in_unison(I_batch, O1_batch, O2_batch)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "logdir = 'logs/word2vec'\n",
    "with tf.Session() as sess:\n",
    "    logwriter = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "    sess.run(init)\n",
    "    for step, (i_batch, o1_batch, o2_batch) in enumerate(train_batch(vnpI, vnpO1, vnpO2, batch_size=batch_size, steps=num_steps)):\n",
    "        sess.run(training_op, feed_dict={\n",
    "                 I: i_batch, O1: o1_batch, O2: o2_batch})\n",
    "        if step % 100 == 0:\n",
    "            summary_str = loss_summary.eval(feed_dict={I: i_batch, O1: o1_batch, O2: o2_batch})\n",
    "            logwriter.add_summary(summary_str, step)\n",
    "    summary_str = loss_summary.eval(feed_dict={I: i_batch, O1: o1_batch, O2: o2_batch})\n",
    "    logwriter.add_summary(summary_str, step)\n",
    "    saver = tf.train.Saver([Wi, Wo1, Wo2])\n",
    "    saver.save(sess, logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(logdir + '/words.tsv', 'w') as words_file:\n",
    "    for i in range(vocab_size):\n",
    "        words_file.write(index_word[i])\n",
    "        words_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 % 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Wo1:0' shape=(10, 3508) dtype=float32_ref>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wo1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul_1:0' shape=(?, 3508) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "So1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(?, 3508) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
